{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbdf45",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Forward Mode Automatic Differentiation\n",
    "## Operator Overloading Approach\n",
    "\n",
    "In this notebook, we will implement **Forward Mode** and **Reverse Mode** from **Automatic Differentiation** using the operator overloading method in Python. The core idea is to define a custom `Variable` class that keeps track of both the value and its derivative (the primal and tangent) and overloads arithmetic operations to propagate derivatives according to calculus rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f31",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ðŸ§® The `Variable` Class (Forward Mode)\n",
    "\n",
    "The `Variable` class in forward mode automatic differentiation represents a value during computation along with its **tangent**, which is the derivative of the function with respect to that variable.\n",
    "\n",
    "It encapsulates two key components:\n",
    "- **`primal`**: the actual numerical value of the variable (used to evaluate the function normally)\n",
    "- **`tangent`**: the value of the derivative of the output with respect to this variable (used to track how a small change in the input affects the output)\n",
    "\n",
    "In forward mode, we compute both the value and derivative **simultaneously** as we evaluate the function. This is particularly efficient when the number of inputs is small compared to the number of outputs.\n",
    "\n",
    "### Operator Overloading\n",
    "\n",
    "We override the standard arithmetic operations (`+`, `-`, `*`, `/`) to support automatic propagation of derivatives. Each operation not only computes the result of the operation but also applies the **chain rule** to update the tangent accordingly.\n",
    "\n",
    "For instance, if `x` and `y` are `Variable` objects:\n",
    "- `x * y` will compute:\n",
    "  - `primal = x.primal * y.primal`\n",
    "  - `tangent = x.tangent * y.primal + y.tangent * x.primal`\n",
    "\n",
    "We also support operations between a `Variable` and a plain Python number, making it seamless to mix constants and variables in expressions.\n",
    "\n",
    "### Elementary Functions\n",
    "\n",
    "Functions like `sin`, `exp`, and `square` are defined specifically for `Variable` inputs. They compute both:\n",
    "- the function value (e.g. `sin(x.primal)`)\n",
    "- the derivative using the chain rule (e.g. `cos(x.primal) * x.tangent` for `sin(x)`)\n",
    "\n",
    "This setup allows us to compute the derivative of any scalar function programmatically, just by initializing the input variables with appropriate tangent values (typically `1.0` for the variable of interest and `0.0` for others).\n",
    "\n",
    "Forward mode is ideal when you're interested in how a function changes with respect to **one** input at a time, such as computing directional derivatives or columns of the Jacobian matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7be92f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import math\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, primal, tangent=0.0):\n",
    "        self.primal = primal\n",
    "        self.tangent = tangent\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Variable(self.primal + other.primal, self.tangent + other.tangent)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Variable(self.primal - other.primal, self.tangent - other.tangent)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Variable(\n",
    "            self.primal * other.primal,\n",
    "            self.tangent * other.primal + other.tangent * self.primal\n",
    "        )\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return Variable(\n",
    "            self.primal / other.primal,\n",
    "            (self.tangent * other.primal - self.primal * other.tangent) / (other.primal**2)\n",
    "        )\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other if isinstance(other, Variable) else Variable(self.primal + other, self.tangent)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other if isinstance(other, Variable) else Variable(self.primal * other, self.tangent * other)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"primal: {self.primal:.3f}, tangent: {self.tangent:.3f}\"\n",
    "        \n",
    "\n",
    "def sin(x):\n",
    "    return Variable(math.sin(x.primal), math.cos(x.primal) * x.tangent)\n",
    "\n",
    "def exp(x):\n",
    "    epx = math.exp(x.primal)\n",
    "    return Variable(epx, epx * x.tangent)\n",
    "\n",
    "def square(x):\n",
    "    return Variable(x.primal**2, 2 * x.primal * x.tangent)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab221f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example 1: Automatic differentiation in forward mode\n",
    "\n",
    "Let's calculate the derivative of the function\n",
    "\n",
    "$$f(x_1, x_2) = \\left[\\sin\\left(\\frac{x_1}{x_2}\\right) + \\frac{x_1}{x_2} - e^{x_2}\\right] \\cdot \\left[\\frac{x_1}{x_2} - e^{x_2}\\right]$$\n",
    "\n",
    "using forward mode, at the point $ (x_1, x_2) = (1.5, 0.5) $ first with respect to $ x_1 $, and then with respect to $ x_2 $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "659578",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def f(x1, x2):\n",
    "    return (sin(x1 / x2) + x1 / x2 - exp(x2)) * (x1 / x2 - exp(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2343",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Calculation of $ \\frac{\\partial f}{\\partial x_1} $ at (1.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e0fa0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(1.5, 0.5) = 2.017\n",
      "âˆ‚f/âˆ‚x1 = 3.012\n"
     ]
    }
   ],
   "source": [
    "x1_val = 1.5\n",
    "x1_tan = 1.0\n",
    "x2_val = 0.5\n",
    "x2_tan = 0.0\n",
    "\n",
    "x1 = Variable(x1_val, x1_tan)  # âˆ‚x1/âˆ‚x1 = 1\n",
    "x2 = Variable(x2_val, x2_tan)  # âˆ‚x2/âˆ‚x1 = 0\n",
    "\n",
    "y = f(x1, x2)\n",
    "print(f\"f({x1_val:.1f}, {x2_val:.1f}) = {y.primal:.3f}\")\n",
    "print(f\"âˆ‚f/âˆ‚x1 = {y.tangent:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Calculation of $ \\frac{\\partial f}{\\partial x_2} $ at (1.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb4ae7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(1.5, 0.5) = 2.017\n",
      "âˆ‚f/âˆ‚x2 = -13.724\n"
     ]
    }
   ],
   "source": [
    "x1_val = 1.5\n",
    "x1_tan = 0.0\n",
    "x2_val = 0.5\n",
    "x2_tan = 1.0\n",
    "\n",
    "x1 = Variable(x1_val, x1_tan)  # âˆ‚x1/âˆ‚x1 = 0\n",
    "x2 = Variable(x2_val, x2_tan)  # âˆ‚x2/âˆ‚x1 = 1\n",
    "\n",
    "y = f(x1, x2)\n",
    "print(f\"f({x1_val:.1f}, {x2_val:.1f}) = {y.primal:.3f}\")\n",
    "print(f\"âˆ‚f/âˆ‚x2 = {y.tangent:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example 2: Automatic differentiation in forward mode\n",
    "\n",
    "Let's calculate the derivative of the function\n",
    "\n",
    "$$\n",
    "g(x_1, x_2, x_3, x_4) = (x_2 \\sin(x_1) + x_2^2, 2 \\, x_3x_4 + x_1) = (r_0, r_1)\n",
    "$$\n",
    "\n",
    "using forward mode, at the point $(x_1, x_2, x_3, x_4) = (1.5, 0.5, 2.0, 3.0)$. This example will focus on getting the Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "785c85",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def g(x1, x2, x3, x4):\n",
    "    r0 = x2 * sin(x1) + square(x2)\n",
    "    r1 = 2 * x3 * x4 + x1\n",
    "    return [r0, r1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c1f7d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def compute_jacobian(f, x_vals):\n",
    "    n_inputs = len(x_vals)\n",
    "    y_sample = f(*[Variable(val, 0.0) for val in x_vals])\n",
    "    n_outputs = len(y_sample)\n",
    "    \n",
    "    jacobian = []\n",
    "\n",
    "    for i in range(n_inputs):\n",
    "        # Set all tangents to 0.0 except for the i-th variable\n",
    "        x_vars = [Variable(val, 1.0 if j == i else 0.0) for j, val in enumerate(x_vals)]\n",
    "        y = f(*x_vars)\n",
    "        jacobian.append([yi.tangent for yi in y])\n",
    "    \n",
    "    # Print primal output and the Jacobian in aligned format with square brackets\n",
    "    print(f\"g({', '.join(map(str, x_vals))}) =\", [yi.primal for yi in y_sample])\n",
    "    print(\"\\nJ_g:\")\n",
    "    for row in list(map(list, zip(*jacobian))):\n",
    "        print(f\"[{', '.join(f'{val: .3f}' for val in row)}]\")\n",
    "\n",
    "    return jacobian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3252a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g(1.5, 0.5, 2.0, 3.0) = [0.7487474933020273, 13.5]\n",
      "\n",
      "J_g:\n",
      "[ 0.035,  1.997,  0.000,  0.000]\n",
      "[ 1.000,  0.000,  6.000,  4.000]\n"
     ]
    }
   ],
   "source": [
    "x_vals = [1.5, 0.5, 2.0, 3.0]\n",
    "J = compute_jacobian(g, x_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43cfb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Reverse Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f47",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The `Variable` Class (Reverse Mode)\n",
    "\n",
    "The `Variable` class in reverse mode automatic differentiation represents a value used during computation and its corresponding **adjoint**, which holds the derivative of a final output with respect to that value.\n",
    "\n",
    "It encapsulates two key attributes:\n",
    "- **`primal`**: the actual value of the variable (used during function evaluation)\n",
    "- **`adjoint`**: the accumulated gradient (derivative of the final result with respect to this variable)\n",
    "\n",
    "In reverse mode, we evaluate the function **forward** (to compute all intermediate values), and then initiate a **reverse pass** (by calling `.backward()`) from the output back through the computation graph, accumulating derivatives into the `adjoint` of each variable.\n",
    "\n",
    "Each arithmetic operation constructs a new `Variable` and assigns it a custom `backward()` function, which:\n",
    "1. Accumulates the incoming gradient into the result variable.\n",
    "2. Computes the local partial derivatives.\n",
    "3. Propagates those gradients to the operand variables using their `backward()` methods.\n",
    "\n",
    "We also define reverse-mode implementations for elementary functions like `sin`, `exp`, and `square`, and include Python operator overloads so expressions like `2 * x` or `x + 3.0` work seamlessly.\n",
    "\n",
    "This setup allows us to compute gradients of scalar outputs with respect to any number of inputs automatically, just by calling `.backward(1.0)` on the final result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "badb05",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import math\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, primal, adjoint=0.0):\n",
    "        self.primal = primal\n",
    "        self.adjoint = adjoint\n",
    "\n",
    "    def backward(self, adjoint):\n",
    "        self.adjoint += adjoint\n",
    "\n",
    "    def __add__(self, other):\n",
    "        variable = Variable(self.primal + other.primal)\n",
    "        def backward(adjoint):\n",
    "            variable.adjoint += adjoint\n",
    "            self.backward(adjoint)\n",
    "            other.backward(adjoint)\n",
    "        variable.backward = backward\n",
    "        return variable\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        variable = Variable(self.primal - other.primal)\n",
    "        def backward(adjoint):\n",
    "            variable.adjoint += adjoint\n",
    "            self.backward(adjoint)\n",
    "            other.backward(-adjoint)\n",
    "        variable.backward = backward\n",
    "        return variable\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        variable = Variable(self.primal * other.primal)\n",
    "        def backward(adjoint):\n",
    "            variable.adjoint += adjoint\n",
    "            self.backward(adjoint * other.primal)\n",
    "            other.backward(adjoint * self.primal)\n",
    "        variable.backward = backward\n",
    "        return variable\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        variable = Variable(self.primal / other.primal)\n",
    "        def backward(adjoint):\n",
    "            variable.adjoint += adjoint\n",
    "            self.backward(adjoint * (1.0 / other.primal))\n",
    "            other.backward(adjoint * (-self.primal / (other.primal ** 2)))\n",
    "        variable.backward = backward\n",
    "        return variable\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + Variable(other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return Variable(other) - self\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * Variable(other)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return Variable(other) / self\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"primal: {self.primal}, adjoint: {self.adjoint}\"\n",
    " \n",
    "\n",
    "def sin(var):\n",
    "    result = Variable(math.sin(var.primal))\n",
    "    def backward(adjoint):\n",
    "        result.adjoint += adjoint\n",
    "        var.backward(adjoint * math.cos(var.primal))\n",
    "    result.backward = backward\n",
    "    return result\n",
    "\n",
    "def exp(var):\n",
    "    result = Variable(math.exp(var.primal))\n",
    "    def backward(adjoint):\n",
    "        result.adjoint += adjoint\n",
    "        var.backward(adjoint * result.primal)\n",
    "    result.backward = backward\n",
    "    return result\n",
    "\n",
    "def square(var):\n",
    "    result = Variable(var.primal ** 2)\n",
    "    def backward(adjoint):\n",
    "        result.adjoint += adjoint\n",
    "        var.backward(adjoint * 2 * var.primal)\n",
    "    result.backward = backward\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59626",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example 1: Automatic differentiation in reverse mode\n",
    "\n",
    "Let's calculate the derivative of the function\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = \\left[\\sin\\left(\\frac{x_1}{x_2}\\right) + \\frac{x_1}{x_2} - e^{x_2}\\right] \\cdot \\left[\\frac{x_1}{x_2} - e^{x_2}\\right]\n",
    "$$\n",
    "\n",
    "using forward mode, at the point $ (x_1, x_2) = (1.5, 0.5) $ first with respect to $ x_1 $, and then with respect to $ x_2 $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ab50a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(1.5, 0.5) = 2.017\n",
      "âˆ‚y/âˆ‚x1 = 3.011843327673907\n",
      "âˆ‚y/âˆ‚x2 = -13.723961509314076\n"
     ]
    }
   ],
   "source": [
    "x1 = Variable(1.5)\n",
    "x2 = Variable(0.5)\n",
    "\n",
    "y = f(x1, x2)\n",
    "y.backward(1.0)\n",
    "\n",
    "print(f\"f({x1.primal:.1f}, {x2.primal:.1f}) = {y.primal:.3f}\")\n",
    "print(f\"âˆ‚y/âˆ‚x1 = {x1.adjoint}\")\n",
    "print(f\"âˆ‚y/âˆ‚x2 = {x2.adjoint}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8f5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example 2: Automatic differentiation in reverse mode\n",
    "\n",
    "Let's calculate the derivative of the function\n",
    "\n",
    "$$\n",
    "g(x_1, x_2, x_3, x_4) = (x_2 \\sin(x_1) + x_2^2, 2 \\, x_3x_4 + x_1) = (r_0, r_1)\n",
    "$$\n",
    "\n",
    "using reverse mode, at the point $(x_1, x_2, x_3, x_4) = (1.5, 0.5, 2.0, 3.0)$. This example will focus on getting the Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "517727",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def g(x1, x2, x3, x4):\n",
    "    r0 = x2 * sin(x1) + square(x2)\n",
    "    r1 = 2 * x3 * x4 + x1\n",
    "    return [r0, r1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dab500",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def compute_jacobian_reverse(f, x_vals):\n",
    "    n_inputs = len(x_vals)\n",
    "\n",
    "    # Wrap raw inputs as Variables (fresh copies for each reverse pass)\n",
    "    def make_vars():\n",
    "        return [Variable(val) for val in x_vals]\n",
    "\n",
    "    # Forward pass to get outputs and output count\n",
    "    vars = make_vars()\n",
    "    outputs = f(*vars)\n",
    "    n_outputs = len(outputs)\n",
    "\n",
    "    jacobian = []\n",
    "\n",
    "    for i in range(n_outputs):\n",
    "        # Reset adjoints (fresh inputs and outputs every time)\n",
    "        vars = make_vars()\n",
    "        outputs = f(*vars)\n",
    "\n",
    "        # Start reverse pass for output i\n",
    "        outputs[i].backward(1.0)\n",
    "\n",
    "        # Collect âˆ‚output_i/âˆ‚xj (j = 0 to n_inputs-1)\n",
    "        jacobian.append([var.adjoint for var in vars])\n",
    "\n",
    "    # Print formatted Jacobian\n",
    "    print(f\"g({', '.join(map(str, x_vals))}) =\", [y.primal for y in outputs])\n",
    "    print(\"\\nJ_g:\")\n",
    "    for row in jacobian:\n",
    "        print(f\"[{', '.join(f'{val: .3f}' for val in row)}]\")\n",
    "\n",
    "    return jacobian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d195e9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g(1.5, 0.5, 2.0, 3.0) = [0.7487474933020273, 13.5]\n",
      "\n",
      "J_g:\n",
      "[ 0.035,  1.997,  0.000,  0.000]\n",
      "[ 1.000,  0.000,  6.000,  4.000]\n"
     ]
    }
   ],
   "source": [
    "x_vals = [1.5, 0.5, 2.0, 3.0]\n",
    "J = compute_jacobian_reverse(g, x_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "537595",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}